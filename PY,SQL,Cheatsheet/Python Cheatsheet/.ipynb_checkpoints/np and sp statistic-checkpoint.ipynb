{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NUMPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Why use NumPy?\n",
    "\n",
    "NumPy arrays are faster and more compact than Python lists. An array consumes less memory and is convenient to use. NumPy uses much less memory to store data and it provides a mechanism of specifying the data types. This allows the code to be optimized even further.\n",
    "\n",
    "What is an array?\n",
    "An array is a central data structure of the NumPy library. An array is a grid of values and it contains information about the raw data, how to locate an element, and how to interpret an element. It has a grid of elements that can be indexed in various ways. The elements are all of the same type, referred to as the array dtype.\n",
    "\n",
    "An array can be indexed by a tuple of nonnegative integers, by booleans, by another array, or by integers. The rank of the array is the number of dimensions. The shape of the array is a tuple of integers giving the size of the array along each dimension.\n",
    "\n",
    "- numpy.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  4  7  8  4  2 11 21]\n",
      "[  0  20  40  60  80 100]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[ 0.   0.5  1.   1.5  2.   2.5  3.   3.5  4.   4.5  5.   5.5  6.   6.5\n",
      "  7.   7.5  8.   8.5  9.   9.5 10.  10.5 11.  11.5 12.  12.5 13.  13.5\n",
      " 14.  14.5 15.  15.5 16.  16.5 17.  17.5 18.  18.5 19.  19.5 20. ]\n"
     ]
    }
   ],
   "source": [
    "# from python list\n",
    "ma_lst = [1,2,4,7,8,4,2,11,21]\n",
    "arr1 = np.array(ma_lst)\n",
    "print(arr1)\n",
    "\n",
    "# generating array using range method\n",
    "ma_arr = np.arange(0,101,20)\n",
    "print(ma_arr)\n",
    "\n",
    "# generating zeros\n",
    "zer_arr = np.zeros((2,5))\n",
    "print(zer_arr)\n",
    "one_arr = np.ones(5)\n",
    "\n",
    "#generating with linearspace\n",
    "lins = np.linspace(0,20,41) #arg1 = start, arg2 = stop, arg3 = how num\n",
    "print(lins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1.dtype #check data typpe\n",
    "arr1.shape # check shape\n",
    "arr1.size #check value size\n",
    "arr.ndim #check dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[94, 12, 76, 33],\n",
       "       [89, 73, 13,  1],\n",
       "       [76, 32, 21, 98],\n",
       "       [26, 84, 59, 63],\n",
       "       [60, 50, 84, 25]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random rand for uniform dist\n",
    "np.random.rand(5,4) # generate a value between 0-1 the arg1 = row, arg2 =column\n",
    "\n",
    "# random randn for normal dist\n",
    "np.random.randn(10) # generate a value near the mean (0) and likely further than 0\n",
    "\n",
    "# random randint\n",
    "np.random.randint(0,100,(5,4)) # generate integer arg1 = low value inclusive, arg2 = high value exclusive, arg3 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15416284, 0.7400497 , 0.26331502, 0.53373939, 0.01457496,\n",
       "       0.91874701, 0.90071485, 0.03342143, 0.95694934, 0.13720932])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random seed\n",
    "np.random.seed(12) #12 is an arbitrary number you can choose anynumber, this 12 is lock a random generator\n",
    "np.random.rand(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "when you determine a seed, the random generator always throw the same value.\n",
    "and repeat the random distribution\n",
    "random seed used in the same cell.\n",
    "if you copy the exact seed, then the value is same with previous cell\n",
    "\n",
    "random seed is useful to test different model, visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5,  6,  7,  8,  9],\n",
       "       [10, 11, 12, 13, 14],\n",
       "       [15, 16, 17, 18, 19],\n",
       "       [20, 21, 22, 23, 24],\n",
       "       [25, 26, 27, 28, 29],\n",
       "       [30, 31, 32, 33, 34],\n",
       "       [35, 36, 37, 38, 39],\n",
       "       [40, 41, 42, 43, 44],\n",
       "       [45, 46, 47, 48, 49]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.arange(5,50)\n",
    "print(arr)\n",
    "arr.reshape(9,5) # you must assign them to a variable to make it a new object"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reshaping note\n",
    "you can reshape an array to any form you like. there is an important rules\n",
    "reshape form must be fit to original size array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1215"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_arr = arr.reshape(9,5)\n",
    "new_arr.max()\n",
    "new_arr.min()\n",
    "new_arr.argmax()\n",
    "new_arr.argmin() #index location\n",
    "new_arr.mean()\n",
    "new_arr.std()\n",
    "new_arr.var()\n",
    "new_arr.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Data in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_arr = np.arange(0,101)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "one can apply python list indexing and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_arr[2]\n",
    "my_arr[:90]\n",
    "my_arr[5:]\n",
    "my_arr[6:20]\n",
    "my_arr[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]\n",
      " [20 21 22 23 24]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8,  9],\n",
       "       [13, 14]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_2d = np.arange(0,25)\n",
    "arr_2d= arr_2d.reshape(5,5)\n",
    "print(arr_2d)\n",
    "#select 19\n",
    "arr_2d[3][-1] #or arr_2d[3,-1]\n",
    "#select subset og 8,9,13,14\n",
    "arr_2d[1:-2,-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 20, 21, 22, 23, 24])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_2d[arr_2d>18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modifying array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,\n",
       "        90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,\n",
       "        90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,\n",
       "        90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_arr[:50] = 90\n",
    "my_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9,  10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,\n",
       "        22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,\n",
       "        35,  36,  37,  38,  39, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "       100, 100])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr4 = np.arange(9,50)\n",
    "new_arr4 = arr4[-10:] #this is a pointer of the original array\n",
    "arr4, new_arr4\n",
    "new_arr4[:] = 100\n",
    "arr4 # the original array being modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy array to avoid original overriding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr4 = np.arange(0,26)\n",
    "new_arr4 = arr4.copy() # explicitly create a copy\n",
    "new_arr4[-10:] = 100 # with this being broadcasted, it wont affect the original array\n",
    "new_arr4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARRAY OPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-134-919cdd352ce4>:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arr5/arr5 #python will still run and replace the invalid div with nan value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr5 = np.arange(0,25).reshape(5,5)\n",
    "arr5 +2\n",
    "arr5 * arr5\n",
    "#here comes the special\n",
    "arr5/arr5 #python will still run and replace the invalid div with nan value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(arr5)\n",
    "np.sin(arr5)\n",
    "np.log(arr5) #this is logaritmic\n",
    "np.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10,  35,  60,  85, 110])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr5.sum(axis=0) #across the row\n",
    "arr5.sum(axis=1) #across the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create column in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'] = np.vectorize(quality)(df['total_bill'],df['tip'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "note that quality must be defined first and it designed to take 2 arg.\n",
    "np vectorize generally will run faster than normal apply function or lambda operation.\n",
    "\n",
    "np vectorize is to transform a function that non numpy aware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistic With Numpy and Scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISTRIBUTION TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform Distribution"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is Uniform Distribution?\n",
    "In statistics, a type of probability distribution in which all outcomes are equally likely. A deck of cards has within it uniform distributions because the likelihood of drawing a heart, a club, a diamond or a spade is equally likely. A coin also has a uniform distribution because the probability of getting either heads or tails in a coin toss is the same.\n",
    "\n",
    "The uniform distribution can be visualized as a straight horizontal line, so for a coin flip returning a head or tail, both have a probability p = 0.50 and would be depicted by a line from the y-axis at 0.50."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "KEY TAKEAWAYS\n",
    "Uniform distributions are probability distributions with equally likely outcomes.\n",
    "There are two types of uniform distributions: discrete and continuous. In the former type of distribution, each outcome is discrete. In a continuous distribution, outcomes are continuous and infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Distribution"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is Normal Distribution?\n",
    "Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "KEY TAKEAWAYS\n",
    "A normal distribution is the proper term for a probability bell curve.\n",
    "In a normal distribution the mean is zero and the standard deviation is 1. It has zero skew and a kurtosis of 3.\n",
    "Normal distributions are symmetrical, but not all symmetrical distributions are normal.\n",
    "In reality, most pricing distributions are not perfectly normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.8\n",
      "85.8\n",
      "8.16\n"
     ]
    }
   ],
   "source": [
    "grades = [88, 82, 85, 84, 90]\n",
    "mean = np.mean(grades)\n",
    "\n",
    "difference_one = (88 - mean) **2\n",
    "difference_two = (82 - mean) **2\n",
    "difference_three = (85 - mean) **2\n",
    "difference_four = (84 - mean) **2\n",
    "difference_five = (90 - mean) **2\n",
    "\n",
    "#Part 1: Sum the differences\n",
    "difference_sum = float(difference_one + difference_two + difference_three+ difference_four + difference_five)\n",
    "\n",
    "#Part 2: Average the differences\n",
    "average_difference = difference_sum / 5\n",
    "print(difference_sum)\n",
    "print(mean)\n",
    "print(average_difference)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Variance In NumPy\n",
    "Well done! You’ve calculated the variance of a data set. The full equation for the variance is as follows:\n",
    "\n",
    "Let’s dissect this equation a bit.\n",
    "\n",
    "Variance is usually represented by the symbol sigma squared.\n",
    "We start by taking every point in the dataset — from point number 1 to point number N — and finding the difference between that point and the mean.\n",
    "Next, we square each difference to make all differences positive.\n",
    "Finally, we average those squared differences by adding them together and dividing by N, the total number of points in the dataset.\n",
    "All of this work can be done quickly using Python’s NumPy library. The var() function takes a list of numbers as a parameter and returns the variance of that dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By finding the number of standard deviations a data point is away from the mean, we can begin to investigate how unusual that datapoint truly is. In fact, you can usually expect around 68% of your data to fall within one standard deviation of the mean, 95% of your data to fall within two standard deviations of the mean, and 99.7% of your data to fall within three standard deviations of the mean."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "quartile, in general a data divide by four section or with three quartile in order to interpret it easily\n",
    "there are q1,q2,q3.\n",
    "median of a data = Q2\n",
    "\n",
    "two methods to define q1 and q3\n",
    "method 1 = exclude q2 to find q1 and q3\n",
    "method 2 = include q2 in both section to find q1 and q3\n",
    "\n",
    "meanwhile q2 is a median of a dataset. numpy has a powerful function to determine q1 and q3\n",
    "with np.quantile(dataset, 0-1) we can determine q1 with 0.25 and q3 with 0.75"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "suppose we want to divide data into 10 slice equally, or as 10 quantile\n",
    "formula :\n",
    "np.quantile(dataset, [ i/10.0 for i in range(1,10)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Nice work! Here are some of the major takeaways about quantiles:\n",
    "\n",
    "Quantiles are values that split a dataset into groups of equal size.\n",
    "If you have n quantiles, the dataset will be split into n+1 groups of equal size.\n",
    "The median is a quantile. It is the only 2-quantile. Half the data falls below the median and half falls above the median.\n",
    "Quartiles and percentiles are other common quantiles. Quartiles split the data into 4 groups while percentiles split the data into 100 groups."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sample Mean and Population Mean\n",
    "Suppose you want to know the average height of an oak tree in your local park. On Monday, you measure 10 trees and get an average height of 32 ft. On Tuesday, you measure 12 different trees and reach an average height of 35 ft. On Wednesday, you measure the remaining 11 trees in the park, whose average height is 31 ft. Overall, the average height for all trees in your local park is 32.8 ft.\n",
    "\n",
    "The individual measurements on Monday, Tuesday, and Wednesday are called samples. A sample is a subset of the entire population. The mean of each sample is the sample mean and it is an estimate of the population mean.\n",
    "\n",
    "Note that the sample means (32 ft., 35 ft., and 31 ft.) were all close to the population mean (32.8 ft.), but were all slightly different from the population mean and from each other.\n",
    "\n",
    "For a population, the mean is a constant value no matter how many times it’s recalculated. But with a set of samples, the mean will depend on exactly what samples we happened to choose. From a sample mean, we can then extrapolate the mean of the population as a whole. There are many reasons we might use sampling, such as:\n",
    "\n",
    "We don’t have data for the whole population.\n",
    "We have the whole population data, but it is so large that it is infeasible to analyze.\n",
    "We can provide meaningful answers to questions faster with sampling.\n",
    "When we have a numerical dataset and want to know the average value, we calculate the mean. For a population, the mean is a constant value no matter how many times it’s recalculated. But with a set of samples, the mean will depend on exactly what samples we happened to choose. From a sample mean, we can then extrapolate the mean of the population as a whole."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keynotes : \n",
    "MORE LARGER THE SAMPLE, MORE ACCURACY IT GETS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hypothesis is premise or claim that we want to test\n",
    "H0, null hipothesis : default or something that already . currently accepted value for a parameter\n",
    "    usually something that stable. such as mean, previous study.\n",
    "H1, alternative hypothesi : a new guy that want to challenge null hypothesis, or new statement that will\n",
    "    replace null hypothesis and involve claims to be tested.\n",
    "\n",
    "general rules of H0 and H1.\n",
    "- they are mathematicaly opposite\n",
    "- H0 is thought to be true until the evidence is enough to reject it. \n",
    "\n",
    "possible outcome of this test/insvestigation:\n",
    "1. we can reject H0, H1 will be the winner.\n",
    "2. fail to reject H0, H0 will remain used.\n",
    "3. type 1 error : rejecting null hipotesis but the reality is True.\n",
    "4. type 2 error : retaining null hipotesis but the reality is False.\n",
    "\n",
    "how do we do the test? we will use test statistic.\n",
    "test statistic is :\n",
    " - calculate from sample data used to decide\n",
    "\n",
    "statisticly significant.\n",
    "    is where we draw the line to help make decission.\n",
    "\n",
    "simple example :\n",
    "suppose a candy machine produce 5 gram of candy in a ten years, and after some workers doing maintenance\n",
    "the candy machine will no longer produce 5 gram anymore.\n",
    "H0 : avg = 5 gram\n",
    "H1 : avg != 5 gram\n",
    "\n",
    "test statistic :\n",
    "- collect 50 sample of candy per day.\n",
    "- get average value\n",
    "- calculate test statistic to help you determine is the data that you have statisticly significant enough\n",
    "    to reject H0 or not.\n",
    "\n",
    "-----------------\n",
    "on monday : avg value is 5.12 grams\n",
    "on wednesday : avg value is 5.72 grams < no longer close to 5 and close to 6 at this point \n",
    "    we start to doubt the H0 is true\n",
    "on friday : avg value is 7.27 grams < far above 5 and personaly i will reject H0 is true\n",
    "\n",
    "with this sampling example, there will be abigous tought and have no standard and based on feeling or perspective to reject the H0 or not.but in statistic have a concrete way to decide when we reject H0 and keep the H0.\n",
    "\n",
    "level of confidence = C : 95% or 99%\n",
    "    how confident we are in our decission. no one gonna believe you if your confidence is only 50 %?\n",
    "    so keep the level of C in 95 % or more.\n",
    "\n",
    "level of significance = alpha : 1- Level of Confidence (95%)\n",
    "                        alpha : 1 - C (0.95)\n",
    "                        alpha : 0.05\n",
    "\n",
    "P-value = probability of obtaining a sample more extreme than the ones observed in your data assuming H0 is true.\n",
    "conclussion of P test.\n",
    "1. if P value <= alpha : reject H0\n",
    "2. if p value > alpha : fail to reject H0\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Assumptions of T-Tests and ANOVA\n",
    "Before we use one or two sample t-tests or ANOVA, we need to be sure that the following things are true:\n",
    "\n",
    "1. The sample(s) should be normally distributed…ish\n",
    "Data analysts in the real world often still perform t-tests or ANOVAs on data that are not normally distributed. This is usually not a problem if sample size is large, but it depends on how non-normal the data is. In general, the bigger the sample size, the safer you are!\n",
    "\n",
    "2. The standard deviations of the samples should be equal\n",
    "For ANOVA and 2-Sample T-Tests, using datasets with standard deviations that are significantly different from each other will often obscure the differences in group means. That said, there is also a way to run a 2-Sample T-Test without assuming equal standard deviations (for example, by setting the equal_var parameter in the scipy.stats.ttest_ind() function equal to False). Running the test in this way has some disadvantages (it essentially makes it harder to reject the null hypothesis even when there is a true difference between groups), so it’s important to check for equal standard deviations before running a test.\n",
    "\n",
    "To check this assumption, it is normally sufficient to divide the two standard deviations and see if the ratio is “close enough” to 1. “Close enough” may differ in different contexts but generally staying within 10% should suffice. This equates to a ratio between 0.9 and 1.1.\n",
    "\n",
    "3. The samples must be independent\n",
    "When comparing two or more datasets, the values in one distribution should not affect the values in another distribution. In other words, knowing more about one distribution should not give you any information about any other distribution.\n",
    "\n",
    "Here are some examples where it would seem the samples are not independent:\n",
    "\n",
    "the number of goals scored per soccer player before, during, and after undergoing a rigorous training regimen\n",
    "a group of patients’ blood pressure levels before, during, and after the administration of a drug\n",
    "It is important to understand your datasets before you begin conducting hypothesis tests on them so that you know you are choosing the right test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STATISTIC FORMULA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "std_formula = variance ** 0.5\n",
    "nba_difference = datapoint - mean\n",
    "num_nba_deviations_distance = nba_difference / nba_standard_deviation\n",
    "    > this is the formula to find the std distance of a piece of data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STATISTIC FUNCTION "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy.stat import ttest_1samp\n",
    "    > for 1 sample H testing\n",
    "syntax = tstat,pval = ttest_1samp(dataset, expected mean(H1))\n",
    "syntax2 = tsat,pval = ttest_ind(data_sample1, data_sample2)\n",
    "    > for testing 2 data sample\n",
    "    > danger or multiple independent 2 sample test is :\n",
    "        error probability, (1 - (0.95 ** number of test being done))\n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "    > ANOVA test\n",
    "syntax = tstat, pval = f_oneway(data_sample1,data_sample2,data_sample3)\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "TUKEY TEST SYNTAX\n",
    "v = np.concatenate([a, b, c])\n",
    "labels = ['a'] * len(a) + ['b'] * len(b) + ['c'] * len(c)\n",
    "\n",
    "tukey_results = pairwise_tukeyhsd(v,labels,0.05)\n",
    "\n",
    "from scipy.stat import binom_test\n",
    "    > binom test\n",
    "syntax : binom_test(x, n, p)\n",
    "x is the number of “successes” (0.059 * 10000 in this case)\n",
    "n is the number of samples (10000 in this case)\n",
    "p is the expected percentage of successes (0.06 in this case)\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "    > chi square test\n",
    "syntax : chi2, pval, dof, expected = chi2_contingency(X)\n",
    "\n",
    "The input to chi2_contingency is a contingency table where:\n",
    "\n",
    "The columns are each a different condition, such as Interface A vs. Interface B\n",
    "The rows represent different outcomes, like “Clicked a Link” vs. “Didn’t Click”\n",
    "This table can have as many rows and columns as you need.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BINOMIAL TEST CASE EXAMPLE\n",
    "\n",
    "Suppose the goal of VeryAnts’s marketing team this quarter was to have 6% of customers click a link that was emailed to them. They sent out a link to 10,000 customers and 510 clicked the link, which comes out to 5.1% instead of 6%. Did they do significantly worse than the target? Let’s use a binomial test to answer this question.\n",
    "\n",
    "Use SciPy’s binom_test function to calculate the p-value the experiment returns for this distribution, where we wanted the mean to be 6% of emails opened, or p=0.06, but only saw 5.1% of emails opened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Test Methods"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                                Numerical\t|   Categorical\n",
    "Sample vs. Known Quantity\t1 Sample T-Test\t|   Binomial Test\n",
    "2 Samples\t                2 Sample T-Test |    Chi Square\n",
    "More Than 2 Samples\t          ANOVA\n",
    "                              and/or\n",
    "                              Tukey\t             Chi Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLICATION OF TEST METHODS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Let’s say that last month 7% of free users of a site converted to paid users, but this month only 5% of free users converted. What kind of test should we use to see if this difference is significant?\n",
    "- ANSWER =  chi square because the argument takes an array. column 1 is month, column 2 is mean of users\n",
    "\n",
    "2.You regularly order delivery from two different Pho restaurants, “What the Pho” and “Pho Tonic”. You want to know if there’s a significant difference between these two restaurants’ average time to deliver to your house. What test could you use to determine this?\n",
    "- ANSWER = 2 sample ttest. because we have a sample of 2 different group and measure its mean difference significant or not\n",
    "\n",
    "3.You just bought a new tea kettle that is supposed to heat water to boiling in 2 minutes. What kind of test can you run to determine if the time-to-boil is averaging significantly more than 2 minutes?\n",
    "- ANSWER = 1 sample ttest because with one sample of one single group \n",
    "\n",
    "You’ve surveyed 10 people who work in finance, 10 people who work in education, and 10 people who work in the service industry on how many cups of coffee they drink per day. What test can you use to determine if there is a significant difference between the average coffee consumption of these three groups?\n",
    "-ANSWER : ANOVA, this test only showing there is a significant difference or not. instead of which pair of sample can rejecet H0\n",
    "\n",
    "5. You own a juice bar and you theorize that 75% of your customers live in the surrounding 5 blocks. You survey a random sample of 12 customers and find that 7 of them live within those 5 blocks. What test do you run to determine if your results significantly differ from your expectation?\n",
    "-ANSWER : BINOMIAL TEST\n",
    "\n",
    "6.You regularly order delivery from two different Pho restaurants, “What the Pho” and “Pho Tonic”. You want to know if there’s a significant difference between these two restaurants’ average time to deliver to your house. What test could you use to determine this?\n",
    "-ANSWER : 2 SAMPLE TTEST\n",
    "\n",
    "7.What kind of test would you use to see if men and women identify differently as “Republican”, “Democrat”, or “Independent”?\n",
    "-ANSWER : CHI SQUARE\n",
    "\n",
    "8.You’ve collected data on 1000 different sites that end with .com, .edu, and .org and have recorded the number of each that have Times New Roman, Helvetica, or another font as their main font. What test can you use to determine if there’s a relationship between top-level domain and font type?\n",
    "- ANSWER : CHI SQUARE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Determination Calculator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "link survey calculator: https://content.codecademy.com/courses/learn-hypothesis-testing/margin_of_error/index.html\n",
    "\n",
    "link AB test sample calculator: https://content.codecademy.com/courses/learn-hypothesis-testing/a_b_sample_size/index4.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2 common sample determination calculator:\n",
    "    a. AB testing sample calculator\n",
    "    b. survey sample calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AB testing sample calculator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keynotes on doing AB testing\n",
    "- before conducting test, set the success indicator from the test. for the best result set only one target. althoough target can be set more than one.\n",
    "    example: - increasing viewer\n",
    "    - increasing engagement\n",
    "    - increasing revenue\n",
    "    - gaining more member to signup\n",
    "    - finding best product version based on above target\n",
    "\n",
    "- set data structure or designing data.\n",
    "    > in SQL might be creating what column to be filled while mining the data\n",
    "    > planning data type to be tested on hypo testing\n",
    "\n",
    "- calculating sample size\n",
    "    > sample size determination can be calculated with this 3 things\n",
    "    1. baseline conversion rate\n",
    "    2. minimum desired lift\n",
    "    3. significant treshold\n",
    "\n",
    "- DANGER OF PREDETERMINED SAMPLE\n",
    "Brian the Product Manager has been running an A/B Test for a redesign of Viral Villanelle’s landing page. Brian used the principles in the Sample Size Determination course on Codecademy to calculate a sample size. He needs 1,100 users to view each variant of the landing page in order to be able to detect his desired lift. When he reaches a total of 2,200 visits to both variants, he runs a Chi-Square test. The new website design performs slightly better, but the results are not statistically significant. Brian decides to run the test for another week to see if he can get to significance. He really wants to launch the redesigned website and he needs statistical validation to show to his boss.\n",
    "\n",
    "Brian has made a big mistake! By choosing to extend the A/B test past the sample size he needs, he introduces personal bias to the results of the test.\n",
    "\n",
    "If the results had already been significant, he wouldn’t have run the test any longer. If he continues this pattern of preferentially extending the test when he wants a different answer, he will be more likely to get the results he wants, regardless if these desired results reflect reality.\n",
    "\n",
    "It’s sad, but Brian will need to accept that the redesigned website isn’t significantly better than the original website.\n",
    "\n",
    "Here are two important rules for making sure that A/B tests remain unbiased:\n",
    "\n",
    "Don’t continue to run the test after the predetermined sample size, until “significant” results are found.\n",
    "Don’t stop a test before reaching the predetermined sample size, just because your results reach significance early (unless there are ethical reasons that require you to stop, like a prescription drug trial).\n",
    "Test data is sensitive to changes in sample size, which is why it is important to calculate beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline conversion rate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The baseline conversion rate is our estimate for the percent of people who will buy a shirt under the current website design. This number may be written as a proportion (eg., .5) or a percent (eg., 50%).\n",
    "\n",
    "We can generally calculate a baseline by looking at historical data for the option that we’re currently using.\n",
    "example: \n",
    "number_of_site_visitors = 2000.0\n",
    "number_of_converted_visitors = 1300.0\n",
    "\n",
    "# Calculate the conversion rate in terms of the above two variables here\n",
    "conversion_rate = number_of_converted_visitors/number_of_site_visitors * 100.0 \n",
    " > 65%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Desired lift"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lift is generally expressed as a percent of the baseline conversion rate. Suppose that 6% of our customers currently buy socks on our website Sock Hops (that’s our baseline conversion rate). We think that a new website layout would increase this. Changing a website layout is hard, so we only think that it’s worth doing if at least 8% of our customers would buy socks on Sock Hops with the new layout. That means that we want to increase our conversions by 2%. To calculate lift:\n",
    "\n",
    "100 * (new - old) / old\n",
    "100 * (8 - 6) / 6\n",
    "33%\n",
    "Sock Hops’ desired lift is 33%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting the sample"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "suppose we have x1 as current headline for our . per day we have 350 users in our site\n",
    "we want to release new headline called x2 and want to see if it is better than the original or the current headline?\n",
    "\n",
    "suppose sample needed calculator shows we need 910 for each version. for somereason we split the traffic for the test to 80 for x1/ 20 for x2\n",
    "\n",
    "each day 350 users, then 80% of it is 210 and 20% of it is 70.\n",
    "to reach 910 users each version. x1 will take 4-5 days, while x2 will take 13 days\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### Sample Size calculator for survey"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3 numbers required for the calculator:\n",
    "a. margin of error\n",
    "b. population size\n",
    "c. confidence level\n",
    "d. likely "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Margin of Error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "margin of errors is a value to represent uncertainty of true result. the smaller margin is the more confident we are and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Population Size"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if population size is not has certain number or cant be predicted or keep increasing.\n",
    "SET THE POPULATION SIZE PARAMS TO 100.000 WILL SUFFICE AND ANY CHANGE ABOVE THAT NUMBER WILL NEGLIGIBLE\n",
    "\n",
    "if population size is certain or well knowed or steady enough\n",
    "SET THE POPULATION SIZE PARAM TO THAT NUMBER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Likely sample proportion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Often, before we conduct a survey, we have a guess of what we expect the results to be. This guess could be based upon the results from a previous survey, or perhaps the results of a small pilot study before the real study.\n",
    "\n",
    "As the expected proportion of people with the desired trait decreases, we can survey fewer people. For example, if we are projecting election results and Candidate C has 1% of the voter base, taking a small sample of only 5 people might be fine, because it is very likely that no one we have chosen is voting for Candidate C. This is close enough to the true proportion.\n",
    "\n",
    "As the expected proportion increases, it is rarer that we hit that proportion accurately with the random sample we choose.\n",
    "\n",
    "If we do not have historical data, we normally use 50%, which gives the most conservative (i.e., largest required) sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using survey calculator to perform AB test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "we want to make a comparison: Are girls more likely to want to learn Python than boys are?\n",
    "\n",
    "This survey is more similar to an A/B Test. Our baseline is the approximate percent of boys who want to learn Python, and our desired lift is the minimum difference between boys and girls that we want to be able to detect.\n",
    "\n",
    "Whenever we want to make comparisons between subpopulations in our survey, we can use the A/B Test Calculator in order to get our desired survey size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
