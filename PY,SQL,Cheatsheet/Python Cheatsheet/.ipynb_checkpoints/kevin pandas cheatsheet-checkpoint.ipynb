{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### first thing waht you do if you want to use pandas module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series in pandas is a one dimentional array and built of numpy array. a series have an index with an addon label.  \n",
    "a series can be indexed with :\n",
    "- numerical  \n",
    "- label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA          1990.0\n",
       "Germany      1942.0\n",
       "Australia    1965.0\n",
       "Rusia        1984.0\n",
       "dtype: float32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1942.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = [1990,1942,1965,1984]\n",
    "idx1 = ['USA','Germany','Australia','Rusia']\n",
    "\n",
    "independent_day = pd.Series(data=data1,index=idx1,dtype='float32')\n",
    "display(independent_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indomi    3000\n",
       "telur     2000\n",
       "aqua      3000\n",
       "sarden    8000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_price= {'indomi':3000,'telur':2000,'aqua':3000,'sarden':8000}\n",
    "food_price2 = {'indomi':3000,'susu':1500,'aqua':3000,'kornet':15000}\n",
    "ser1 = pd.Series(food_price)\n",
    "ser2 = pd.Series(food_price2)\n",
    "ser1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['indomi', 'telur', 'aqua', 'sarden'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independent_day[2] #1965\n",
    "independent_day['Germany'] #1942\n",
    "\n",
    "#grabbing index name or keys\n",
    "food_price.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Series Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aqua       6000.0\n",
       "indomi     6000.0\n",
       "kornet    15000.0\n",
       "sarden     8000.0\n",
       "susu       1500.0\n",
       "telur      2000.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser1 * 2\n",
    "ser1 + ser2 #this operation will show nan if there is any mismatch\n",
    "ser1.add(ser2,fill_value = 0) #with pressing tab, it will show any operation. with this addition ops will sum and keep the mismatch value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATAFRAME"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a group of panda series that shares same index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating dataframe\n",
    "there is a few way to create data frame.\n",
    "1. call DataFrame function from pandas module\n",
    "2. create dataframe using dictionary data type, but column name will placed in alphabetically order\n",
    "3. using column keyword argument with list datatype\n",
    "4. using pandas module to read other source. use function (pd.read_csv(\"somefile.csv\"))\n",
    "    - first line data in comma separated value is a column name, and the other lines is the value of a column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jan</th>\n",
       "      <th>Feb</th>\n",
       "      <th>Mar</th>\n",
       "      <th>Apr</th>\n",
       "      <th>May</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>JKT</th>\n",
       "      <td>99</td>\n",
       "      <td>90</td>\n",
       "      <td>15</td>\n",
       "      <td>95</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BDG</th>\n",
       "      <td>90</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>75</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BGR</th>\n",
       "      <td>71</td>\n",
       "      <td>34</td>\n",
       "      <td>96</td>\n",
       "      <td>40</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YOG</th>\n",
       "      <td>90</td>\n",
       "      <td>26</td>\n",
       "      <td>83</td>\n",
       "      <td>16</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SLE</th>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>98</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Jan  Feb  Mar  Apr  May\n",
       "JKT   99   90   15   95   28\n",
       "BDG   90    9   20   75   22\n",
       "BGR   71   34   96   40   85\n",
       "YOG   90   26   83   16   62\n",
       "SLE   16    7   98    6   26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = pd.DataFrame({\n",
    "  'Product ID': [1, 2, 3, 4],\n",
    "  'Product Name':['t-shirt','t-shirt','skirt','skirt'],\n",
    "  'Color':['blue','green','red','black']\n",
    "})\n",
    "df2 = pd.DataFrame([\n",
    "  [1, 'San Diego', 100],\n",
    "  [2, 'Los Angeles', 120],\n",
    "  [3,'San Francisco', 90],\n",
    "  [4, 'Sacramento',115]\n",
    "],\n",
    "  columns=[\n",
    "    'Store ID',\n",
    "    'Location',\n",
    "    'Number of Employees'\n",
    "  ])\n",
    "\n",
    "np.random.seed(20)\n",
    "data2 = np.random.randint(0,101,(5,5))\n",
    "idx2 = ['JKT','BDG','BGR','YOG','SLE']\n",
    "col2 = ['Jan','Feb','Mar','Apr','May']\n",
    "\n",
    "df = pd.DataFrame(data = data2,index = idx2,columns=col2)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 9EC7-490F\n",
      "\n",
      " Directory of C:\\Users\\ACER\n",
      "\n",
      "22/12/2020  17:44    <DIR>          .\n",
      "22/12/2020  17:44    <DIR>          ..\n",
      "19/04/2019  19:31    <DIR>          .android\n",
      "12/04/2019  11:14    <DIR>          .AndroidStudio3.3\n",
      "25/10/2020  20:31               893 .bash_history\n",
      "24/12/2018  20:33    <DIR>          .cache\n",
      "22/12/2020  17:44    <DIR>          .conda\n",
      "22/12/2020  13:50                43 .condarc\n",
      "16/06/2020  23:02    <DIR>          .dbus-keyrings\n",
      "09/09/2020  22:59    <DIR>          .dotnet\n",
      "19/04/2019  19:30                16 .emulator_console_auth_token\n",
      "18/11/2020  22:54               180 .gitconfig\n",
      "12/04/2019  12:36    <DIR>          .gradle\n",
      "30/09/2020  22:28    <DIR>          .idlerc\n",
      "16/12/2020  23:41    <DIR>          .ipynb_checkpoints\n",
      "13/09/2020  08:12    <DIR>          .ipython\n",
      "19/10/2020  10:32    <DIR>          .jupyter\n",
      "18/04/2018  08:30    <DIR>          .LINE\n",
      "14/10/2020  13:28    <DIR>          .matplotlib\n",
      "02/10/2016  07:26    <DIR>          .oracle_jre_usage\n",
      "18/04/2018  08:30    <DIR>          .QtWebEngineProcess\n",
      "09/06/2020  11:43    <DIR>          .thumbnails\n",
      "09/09/2020  22:28    <DIR>          .vscode\n",
      "14/10/2020  15:25               119 codecademy.json\n",
      "12/06/2019  19:43    <DIR>          Contacts\n",
      "31/03/2017  16:27    <DIR>          Creative Cloud Files\n",
      "20/12/2020  20:04    <DIR>          Data science path\n",
      "20/12/2020  17:02    <DIR>          Desktop\n",
      "26/10/2020  17:33    <DIR>          Documents\n",
      "22/12/2020  07:45    <DIR>          Downloads\n",
      "05/12/2020  18:32    <DIR>          ebook\n",
      "12/06/2019  19:43    <DIR>          Favorites\n",
      "05/12/2020  18:33    <DIR>          Interview test KMK online\n",
      "31/10/2020  22:13             8.828 kevin algorithm.ipynb\n",
      "17/11/2020  22:57           352.901 kevin matplotlib cheatsheet.ipynb\n",
      "22/12/2020  17:44            33.715 kevin pandas cheatsheet.ipynb\n",
      "21/12/2020  23:55            24.943 kevin py cheatsheet.ipynb\n",
      "27/03/2016  01:29    <DIR>          kevinelri@ymail.com Creative Cloud Files\n",
      "30/11/2020  15:16            12.436 language chart horizontal.png\n",
      "30/11/2020  15:16            14.191 language chart.png\n",
      "21/12/2020  23:55           179.008 learning terminal.ipynb\n",
      "12/06/2019  19:43    <DIR>          Links\n",
      "26/08/2020  10:07    <DIR>          Music\n",
      "11/12/2020  15:06            45.159 my other skills stat.png\n",
      "30/11/2020  19:48            69.172 my skill stats.png\n",
      "25/10/2020  19:41    <DIR>          my-learing-project\n",
      "21/12/2020  23:54            44.618 np and sp statistic.ipynb\n",
      "26/08/2020  10:16    <DIR>          Pictures\n",
      "19/10/2020  10:30                 0 pokemon.py\n",
      "03/11/2020  18:21    <DIR>          project AB testing shoeflycom\n",
      "19/11/2020  20:11    <DIR>          project biodiversity\n",
      "03/11/2020  18:26    <DIR>          project coded correspondence\n",
      "03/11/2020  18:25    <DIR>          project elections\n",
      "14/11/2020  21:00    <DIR>          project familiar blood\n",
      "18/11/2020  18:53    <DIR>          project farmburgs AB test\n",
      "14/11/2020  23:30    <DIR>          project fetchmaker statistic\n",
      "17/12/2020  20:30    <DIR>          Project hurricane\n",
      "11/11/2020  15:59    <DIR>          project Life Expectancy By Country\n",
      "16/11/2020  22:17    <DIR>          project nish Mish mosh\n",
      "07/11/2020  21:54    <DIR>          project orion constellation\n",
      "04/11/2020  17:34    <DIR>          project Page Visits Funnel cooltshirts\n",
      "03/11/2020  18:24    <DIR>          project pasta bazoolin\n",
      "03/11/2020  18:25    <DIR>          project petal power\n",
      "19/10/2020  22:46             6.334 project pokemon.py.ipynb\n",
      "03/11/2020  18:27    <DIR>          project reggie linnear reggression\n",
      "03/11/2020  18:28    <DIR>          project sortation\n",
      "05/11/2020  21:55    <DIR>          project Sublime Limes Line Graphs\n",
      "08/11/2020  21:07    <DIR>          project variance in weather\n",
      "27/03/2016  06:49    <DIR>          pucenongan@gmail.com Creative Cloud Files\n",
      "04/10/2020  21:47    <DIR>          PycharmProjects\n",
      "15/09/2020  16:01    <DIR>          python-random-quote\n",
      "09/11/2020  20:16    <DIR>          quartile lesson python\n",
      "12/06/2019  19:43    <DIR>          Saved Games\n",
      "12/06/2019  19:43    <DIR>          Searches\n",
      "09/09/2020  23:43    <DIR>          source\n",
      "10/11/2020  23:46    <DIR>          SQL project\n",
      "18/06/2016  05:50    <DIR>          Tracing\n",
      "26/08/2020  10:38    <DIR>          Videos\n",
      "              17 File(s)        792.556 bytes\n",
      "              61 Dir(s)  66.023.505.920 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ACER'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pwd is to know where are you right now in the directory\n",
    "ls to know what is available file in you current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting data frames"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. use pd.head(num)\n",
    "    using head function will show 5 first row and its column by default, if want more data tobe shown pass integer \n",
    "    as an argument in head function\n",
    "2. use pd.info()\n",
    "    The method df.info() gives some statistics for each column.    \n",
    "3. display(dataframes)\n",
    "    this can show to the terminal a database given in the argument\n",
    "4. print(dataframes)\n",
    "    old school method to print out the dataframes\n",
    "5. use df.columns\n",
    "    > will return a list containing column names\n",
    "6. use df.index\n",
    "    > will return information of index from dataset\n",
    "7. use df.values\n",
    "    > will return an array containing all values from dataset\n",
    "8. use df.shape\n",
    "    > return information of how dataset formed\n",
    "9.use df.describe()\n",
    "    > return general statistic of the dataframe, some of them may not accurate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options for handling missing value.\n",
    "1. Keep it\n",
    "2. Remove it\n",
    "3. Replace it\n",
    "\n",
    "there is no fixed answer how to handle missing value, becaue not all of the condition we use same option.  \n",
    "each of the option has pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. keep the missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> pros:\n",
    ">> Easiest to do  \n",
    ">> does not manipulate or change the true data\n",
    "\n",
    "> cons:  \n",
    ">> many method doesnt support Nan  \n",
    ">> oftenly there is a reasonable guesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. dropping missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pros :\n",
    "- easy to do\n",
    "- can be based on rules\n",
    "\n",
    "cons :\n",
    "- potential to lose a lot of data and useful information\n",
    "- limit trained model for future data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "when dropping a missing value can be done with drop row or drop column.\n",
    "oftenly is a good idea when dropping a row, one calculate the dropping percentage.\n",
    "\n",
    "it is a good choice to drop a column if every row is missing the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Replace a missing value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pros :\n",
    "- potentiallly can save a lot of data to be used in model training\n",
    "cons :\n",
    "- hardest to do\n",
    "- somewhat it is arbitrary\n",
    "- because it is a made up value, that can be trigger false conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check missing value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.isnull()\n",
    "df.notnull()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df[df['column'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining standard missing value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "suppose we have messy data that indicate a lot of missing value. the missing value might be (none,nan,n/a,null,-)\n",
    "\n",
    "step 1. we must define what value considered as a missing value.\n",
    "missing_value ['null','Nan','-','n/a','none','',' ']\n",
    "\n",
    "step 2. give the the argument with our definition of missing value\n",
    "df = pd.read_csv('city_data',na_values=missing_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop missing value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.dropna()\n",
    "tresh = how many non null value minimum until being dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling null value with data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Entire Data frame\n",
    "df.fillna('some value')\n",
    "species.fillna('No Intervention', inplace=True)\n",
    "    > fill every NaN with No Intervention, and apply to self dataframes.\n",
    "\n",
    "2. specific column with pandas\n",
    "df['DataFrame Column'] = df['DataFrame Column'].fillna(0)\n",
    "\n",
    "3. specific column with numpy\n",
    "df['DataFrame Column'] = df['DataFrame Column'].replace(np.nan, 0)\n",
    "\n",
    "4.entire dataframe with numpy\n",
    "df.replace(np.nan,0)\n",
    "\n",
    "5. fill missing value with median or something else\n",
    "    > median = df['column_tofill'].median()\n",
    "    > df['column_tofill'].fillna(median,inplace=True)\n",
    "\n",
    "6. fill with interpolation\n",
    "pd.interpolate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unexpected value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "suppose column type is string, but some of it somehow having integers.\n",
    "\n",
    "this is how to fix it\n",
    "\n",
    "counter = 0\n",
    "for row in df.string_column:\n",
    "    try:\n",
    "        int(row) # trying to convert row values into integers if success :\n",
    "        df.loc[counter, 'string_column'] = np.nan #if condition above success then the value will become NaN\n",
    "    except ValueError: #if failing to convert into integers then:\n",
    "        pass #do nothing continue looping\n",
    "    counter +=1 #counter for iterate through rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicate Value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### how to deal with date and times in pandas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "there are many ways to deal with date type datas.\n",
    "1. using df['time'] = pd.to_datetime(df.time)\n",
    "2. using df = pd.read_csv('file.csv', parse_dates=[column index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### selecting data "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. selecting all value from one column\n",
    "    a. use dataframe['column']\n",
    "        this method used if the column name break the variable name rules in pyhton. example : 12shoes, red fruit\n",
    "    b. df.columnname\n",
    "        this method used if column name follow variable rules in python\n",
    "        \n",
    "2. selecting value from multiple\n",
    "   > new_df = dataframe[['column1', 'column2']]\n",
    "       *Note: *Make sure that you have a double set of brackets ([[]]), or this command won’t work!\n",
    "       \n",
    "3. selecting rows\n",
    "    > dataframes.iloc[index]\n",
    "        DataFrames are zero-indexed, meaning that we start with the 0th row and count up from there\n",
    "        iloc is the function to select particular row\n",
    "    \n",
    "    #selecting multiple rows\n",
    "    > dataframes.iloc[start index:end index]\n",
    "        iloc can be passed by 2 argument first is row argument, and column argument\n",
    "        df.iloc[2:4,3:5] \n",
    "            this code will show row 2,4 and column 3,5\n",
    "    > dataframes.loc[]\n",
    "        loc() label based data selecting method which means that we have to pass \n",
    "        the name of the row or column which we want to select. This method includes \n",
    "        the last element of the range passed in it, unlike iloc(). loc() can accept \n",
    "        the boolean data unlike iloc().\n",
    "        example of loc functions:\n",
    "            data.loc[(data.Brand == 'Maruti') & (data.Mileage > 25)] #will select record with given filter condition.\n",
    "            data.loc[(data.Year < 2015), ['Mileage']] = 22 #will update record where year below 2015 and column name mileage\n",
    "            to the value 22.\n",
    "    \n",
    "    #selecting rows with logic\n",
    "    > df[df.MyColumnName #expression# #value#]\n",
    "        expression operator in python (<,>,,<=,>=,==,!=), (& 'and',| 'or')\n",
    "        python will scan the value mathces with logic expression\n",
    "    \n",
    "    > df[(df.age < 30) #condition# (df.name == 'Martha Jones')]\n",
    "       multiple expression separated by parentheses, then condition operator\n",
    "       \n",
    "    > df[df.gender.isin(['male'])]\n",
    "        #this will show all records with gender male\n",
    "        \n",
    "    > filter1 = dataframe[\"Gender\"].isin([\"Female\"]) \n",
    "            gender is a column from dataframe, female is the record in gender.\n",
    "            the result is showing all records with female gender\n",
    "      filter2 = dataframe[\"Team\"].isin([\"Engineering\", \"Distribution\", \"Finance\" ])\n",
    "              team is column, engineering and etc are records from team column.\n",
    "              the result is showing all record that have value engineering and etc.\n",
    "      queries = dataframe[filter1 & filer2]\n",
    "          # displaying data with both filter applied and mandatory \n",
    "          \n",
    "    #select record in particular column with condition\n",
    "    > df.loc[(df.name == 'kevin'), ['name','gender']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selecting multilevel index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "suppose we have 2 level index\n",
    "\n",
    "df.loc[80]\n",
    "> grab the outer index\n",
    "df.loc[(70,4)]\n",
    "> must use tuple. first arg is outer level, 2nd arg is inner level"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.xs(keys,level)\n",
    "> keys is your index\n",
    "> level is in which level the index is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the largest and smallest"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.nlargest(2,'column')\n",
    "> arg 1 how many row\n",
    "> what column\n",
    "\n",
    "df.nsmallest(arg1,arg2)\n",
    "\n",
    "both of these methods is the same with\n",
    "df.sort_values('col',ascending=True).iloc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Values in between"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.column.between(10,20,inclusive=True)\n",
    "> return boolean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "protection_counts = species.groupby('conservation_status')\\\n",
    "    .scientific_name.nunique().reset_index()\\\n",
    "    .sort_values(by='scientific_name')\n",
    "\n",
    "sort_values argument :\n",
    "by: Single/List of column names to sort Data Frame by.\n",
    "axis: 0 or ‘index’ for rows and 1 or ‘columns’ for Column.\n",
    "ascending: Boolean value which sorts Data frame in ascending order if True.\n",
    "inplace: Boolean value. Makes the changes in passed data frame itself if True.\n",
    "kind: String which can have three inputs(‘quicksort’, ‘mergesort’ or ‘heapsort’) of algorithm used to sort data frame.\n",
    "na_position: Takes two string input ‘last’ or ‘first’ to set position of Null values. Default is ‘last’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### slicing indices"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "when we want to select particular rows or some records and save it into a variable,\n",
    "the index are messed up and not numerically in order.\n",
    "this is the pandas function comes in handy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.reset_index(inplace=True,drop =True)\n",
    "    reset_index() function is powerfull to rearrange index into numerically order.\n",
    "    reset_index has few arguments, 2 of that is:\n",
    "        inplace= boolean value, this argument will create new column that contains index in numberically orders\n",
    "            inplace will modify our existing column\n",
    "        drop = boolean value, this argument will delete new index column and set to base index label if combined\n",
    "            both with inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## updating dataframe"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                                                ##CONSIDER THIS IS DATA FRAMES##\n",
    "df = pd.DataFrame([\n",
    "  [1, '3 inch screw', 0.5, 0.75],\n",
    "  [2, '2 inch nail', 0.10, 0.25],\n",
    "  [3, 'hammer', 3.00, 5.50],\n",
    "  [4, 'screwdriver', 2.50, 3.00]\n",
    "],\n",
    "  columns=['Product ID', 'Description', 'Cost to Manufacture', 'Price']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### some way to add column into existing dataframe"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. creating column with value match the existing rows\n",
    "df['Sold in Bulk?']=['Yes','Yes','No','No']\n",
    "    > this is update syntax that add sold in bulk new column into dataframe with value of yes and no \n",
    "2. creating column with single value without bracket.\n",
    "df['Is taxed?'] = 'Yes'\n",
    "    > if no square bracket given to the value, then all of the record for added column will result 'Yes'   \n",
    "3. creating column with operation.\n",
    "df['Margin']= df.Price - df['Cost to Manufacture']\n",
    "    > this will create margin column with value of price value - cost to manuf value for each records in margin column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create column with lambda operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality(col1,col2):\n",
    "    if col2/col1 > .25:\n",
    "        return 'Generous'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df['Quality'] = df[['total_bill','tip']].apply \\\n",
    "(lambda df: quality(df['total_bill'],df['tip']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create column with np vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'] = np.vectorize(quality)(df['total_bill'],df['tip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### column operations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. applying function into column to create new column\n",
    "df['Lowercase Name']= df.Name.apply(function)\n",
    "    > syntax above will create new column named lowecase name in df dataframe, that contains value\n",
    "    of lower function from Name column\n",
    "    > function apply() will implement called function into column\n",
    "    \n",
    "2. applying logic operation to create new column\n",
    "orders['shoe_source'] = orders.shoe_material.apply(lambda x: 'animal' if x == 'leather'else 'vegan')\n",
    "    >\n",
    "\n",
    "3.orders['salutation'] = orders.apply(lambda x: \"Dear Mr. {}\".format(x['last_name']) if x['gender'] == 'male' \\\n",
    "else \"Dear Ms. {}\".format(x['last_name']), axis=1 )\n",
    "\n",
    "4.df['is_purchase'] = df.click_day.apply(lambda x: 'No Purchase' if pd.isnull(x) else 'Purchase')\n",
    "5. df['new column'] = df['col1'] + df['col2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. replace known value\n",
    "df['col'].replace(['value1','value2'],['new_val1','new_val2'])\n",
    "my_map = {'val1':'newval1','val2':'newval2'}\n",
    "df.col.map(my_map)\n",
    "\n",
    "2. replace value based on conditional\n",
    "combined_df.loc[combined_df.name == 'Jack', 'name'] = 'Jock'\n",
    "combined_df['name'] = np.where(combined_df.name == 'John','Jhen',combined_df.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.drop(labels=None,\n",
    "    axis=0,\n",
    "    index=None,\n",
    "    columns=None,\n",
    "    level=None,\n",
    "    inplace=False,\n",
    "    errors='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Index label"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.set_index(keys,\n",
    "    drop=True,\n",
    "    append=False,\n",
    "    inplace=False,\n",
    "    verify_integrity=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### renaming column"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. method 1\n",
    "df.columns = ['newcolumn name']\n",
    "    > remember, the syntax is with plural column with S\n",
    "    > with this method you cant change specific column to be replaced\n",
    "        and if mispelled column name would be a mess\\\n",
    "    > with this method you cant change just only one column name. which means\n",
    "        all of the column name must be replaced\n",
    "\n",
    "2. method 2\n",
    "df.rename(columns={'old':'new'},inplace = True)\n",
    "    > using rename method takes 2 argument, columns and inplace.\n",
    "        inplace means you will modify original dataframe not createing new one\n",
    "    > using dictionary types, each column separated by comma\n",
    "    > this method give more flexibility\n",
    "\n",
    "3. method 3\n",
    "df2.columns = df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usefull aggregation syntax"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2.agg(['mean','std'])[['mpg','weight']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2.agg({'mpg':['mean','std'],'weight':['mean','std'],'displacement':['mean','max']})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mean = Average of all values in column\n",
    "std\t= Standard deviation\n",
    "median = Median\n",
    "max\t= Maximum value in column\n",
    "min\t= Minimum value in column\n",
    "count =Number of values in column\n",
    "nunique = Number of unique values in column\n",
    "unique = List of unique values in column\n",
    "sum = ..\n",
    "idxmax = show index of max value\n",
    "\n",
    "        HOW TO USE\n",
    "dataframe.column.aggregate()\n",
    "\n",
    "GROUP BY aggregate\n",
    "df.groupby('column1').column2.measurement()\n",
    "\n",
    "        HOW TO USE\n",
    "df.groupby('student').grade.mean()\n",
    "    > this tell, find the average of grade column and group it to student column.\n",
    "    > if this code being run, type of this data will be a panda series not a dataframe\n",
    "    \n",
    "orders.groupby('shoe_type').price.max().reset_index()\n",
    "    > with reset_index() function called by, the data type will be a dataframe\n",
    "\n",
    "MULTI COLUMN GROUPBY\n",
    "shoe_counts = orders.groupby(['shoe_type','shoe_color']).id.count().reset_index()\n",
    "    > the problems are we want to know what is the corelation between shoe type/shoe color\n",
    "        with the amount of item sold.\n",
    "    > generaly the code said count how many records or how many products are sold,\n",
    "        and group it to column shoe type and shoe color.\n",
    "    > reset index function to create a new data frame\n",
    "    > after that code run, might be little bit uncomfortable due to column name. \n",
    "    (data frame column name is id, not a descriptive column name).\n",
    "    in order to get rid of that, we need to rename the column name\n",
    "     with syntax  >>>>> orders.rename(columns={\"id\": \"counts\"})\n",
    "\n",
    "multimodule measurement operation PANDAS X NUMPY\n",
    "The syntax will be: \n",
    "df.groupby('category').wage.apply(lambda x: np.percentile(x, 75)).reset_index()\n",
    "     > speaking in genereal, this code will show 75% percentile of an employee wage and group it by each departement category\n",
    "     > with .apply() called, will relace measurement function to achieve more flexibility\n",
    "     > using lambda anonymous fuction within aplly function\n",
    "     > x in lambda argument refer to wage column\n",
    "     > reset_index() function will create a dataframe, not a series\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas statistic"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# most of this at pandas aggregate cell\n",
    "df.corr()\n",
    "> return correlation table of dataframes\n",
    "\n",
    "df.column.value_counts()\n",
    "return number of categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIVOTING DATA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "main goals of pivoting data is to achieve better understanding what is going on\n",
    "or what is the correlation inbetween a data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.pivot(columns='ColumnToPivot',\n",
    "         index='ColumnToBeRows',\n",
    "         values='ColumnToBeValues'),reset_index()\n",
    "         \n",
    "columns is horizontal column\n",
    "index is vertical column\n",
    "value is amount of data which indicates realation inbetween horizontal and vertical column\n",
    "\n",
    "df.transpose()\n",
    "> change the column into row and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Table operations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "method 1.\n",
    "new_df = pd.merge(table1, table2)\n",
    "    > using pandas module call\n",
    "\n",
    "method 2\n",
    "new_df = orders.merge(customers)\n",
    "    > using table1 then call merge function\n",
    "    \n",
    "big_df = orders.merge(customers).merge(products)\n",
    "    > chaining method 2 "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in case if we have same column name but each other refer to different entities. \n",
    "we would like to use this method.\n",
    "\n",
    "pd.merge(\n",
    "    orders,\n",
    "    customers,\n",
    "    left_on='customer_id',\n",
    "    right_on='id',\n",
    "    suffixes=['_order', '_customer']\n",
    ")\n",
    "\n",
    "suppose we have same column name called 'id'. we can separate the column name with left_on \n",
    "that refer to orders table, right_on refer to customers table. without suffixes argument,\n",
    "pandas create column named id_x (refer to orders id column) id_y (refer to customers id column)\n",
    "suffixes argument will create a template for column id with given keyword argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### outer join"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.merge(table1, table2, how='outer')\n",
    "    >if there is missing value from ond in anothe table, it wont be erased and showing none/nan instead.\n",
    "    > this method did not erase column if there is any mismatch column\n",
    "\n",
    "pd.merge(company_a, company_b, how='left')\n",
    "    >A Left Merge includes all rows from the first (left) table, but only rows from the second (right) table that match the first table.\n",
    "    > in general, the records show all of company_a rows, and in company_b records will show nan because there is no value related to company_a.\n",
    "    > some row that company_a doesnt have while company_b have it, the table wont show company_b properties because the focus is on the left (company_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Rows"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " pd.concat([bakery,ice_cream],axis=1)\n",
    "     > this will work if both DF have the same column name and the same column size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.concat([df1,df2],axis=0)\n",
    "    > note this only work if both df has the same row size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.sample(num, frac)\n",
    "> num is number of random row\n",
    "> frac is percentage of dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
