{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NUMPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Why use NumPy?\n",
    "\n",
    "NumPy arrays are faster and more compact than Python lists. An array consumes less memory and is convenient to use. NumPy uses much less memory to store data and it provides a mechanism of specifying the data types. This allows the code to be optimized even further.\n",
    "\n",
    "What is an array?\n",
    "An array is a central data structure of the NumPy library. An array is a grid of values and it contains information about the raw data, how to locate an element, and how to interpret an element. It has a grid of elements that can be indexed in various ways. The elements are all of the same type, referred to as the array dtype.\n",
    "\n",
    "An array can be indexed by a tuple of nonnegative integers, by booleans, by another array, or by integers. The rank of the array is the number of dimensions. The shape of the array is a tuple of integers giving the size of the array along each dimension.\n",
    "\n",
    "- numpy.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  4  7  8  4  2 11 21]\n",
      "[  0  20  40  60  80 100]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[ 0.   0.5  1.   1.5  2.   2.5  3.   3.5  4.   4.5  5.   5.5  6.   6.5\n",
      "  7.   7.5  8.   8.5  9.   9.5 10.  10.5 11.  11.5 12.  12.5 13.  13.5\n",
      " 14.  14.5 15.  15.5 16.  16.5 17.  17.5 18.  18.5 19.  19.5 20. ]\n"
     ]
    }
   ],
   "source": [
    "# from python list\n",
    "ma_lst = [1,2,4,7,8,4,2,11,21]\n",
    "arr1 = np.array(ma_lst)\n",
    "print(arr1)\n",
    "\n",
    "# generating array using range method\n",
    "ma_arr = np.arange(0,101,20)\n",
    "print(ma_arr)\n",
    "\n",
    "# generating zeros\n",
    "zer_arr = np.zeros((2,5))\n",
    "print(zer_arr)\n",
    "one_arr = np.ones(5)\n",
    "\n",
    "#generating with linearspace\n",
    "lins = np.linspace(0,20,41) #arg1 = start, arg2 = stop, arg3 = how num\n",
    "print(lins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1.dtype #check data typpe\n",
    "arr1.shape # check shape\n",
    "arr1.size #check value size\n",
    "arr.ndim #check dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[94, 12, 76, 33],\n",
       "       [89, 73, 13,  1],\n",
       "       [76, 32, 21, 98],\n",
       "       [26, 84, 59, 63],\n",
       "       [60, 50, 84, 25]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random rand for uniform dist\n",
    "np.random.rand(5,4) # generate a value between 0-1 the arg1 = row, arg2 =column\n",
    "\n",
    "# random randn for normal dist\n",
    "np.random.randn(10) # generate a value near the mean (0) and likely further than 0\n",
    "\n",
    "# random randint\n",
    "np.random.randint(0,100,(5,4)) # generate integer arg1 = low value inclusive, arg2 = high value exclusive, arg3 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15416284, 0.7400497 , 0.26331502, 0.53373939, 0.01457496,\n",
       "       0.91874701, 0.90071485, 0.03342143, 0.95694934, 0.13720932])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random seed\n",
    "np.random.seed(12) #12 is an arbitrary number you can choose anynumber, this 12 is lock a random generator\n",
    "np.random.rand(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "when you determine a seed, the random generator always throw the same value.\n",
    "and repeat the random distribution\n",
    "random seed used in the same cell.\n",
    "if you copy the exact seed, then the value is same with previous cell\n",
    "\n",
    "random seed is useful to test different model, visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5,  6,  7,  8,  9],\n",
       "       [10, 11, 12, 13, 14],\n",
       "       [15, 16, 17, 18, 19],\n",
       "       [20, 21, 22, 23, 24],\n",
       "       [25, 26, 27, 28, 29],\n",
       "       [30, 31, 32, 33, 34],\n",
       "       [35, 36, 37, 38, 39],\n",
       "       [40, 41, 42, 43, 44],\n",
       "       [45, 46, 47, 48, 49]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.arange(5,50)\n",
    "print(arr)\n",
    "arr.reshape(9,5) # you must assign them to a variable to make it a new object"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reshaping note\n",
    "you can reshape an array to any form you like. there is an important rules\n",
    "reshape form must be fit to original size array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1215"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_arr = arr.reshape(9,5)\n",
    "new_arr.max()\n",
    "new_arr.min()\n",
    "new_arr.argmax()\n",
    "new_arr.argmin() #index location\n",
    "new_arr.mean()\n",
    "new_arr.std()\n",
    "new_arr.var()\n",
    "new_arr.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Data in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_arr = np.arange(0,101)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "one can apply python list indexing and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_arr[2]\n",
    "my_arr[:90]\n",
    "my_arr[5:]\n",
    "my_arr[6:20]\n",
    "my_arr[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]\n",
      " [20 21 22 23 24]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8,  9],\n",
       "       [13, 14]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_2d = np.arange(0,25)\n",
    "arr_2d= arr_2d.reshape(5,5)\n",
    "print(arr_2d)\n",
    "#select 19\n",
    "arr_2d[3][-1] #or arr_2d[3,-1]\n",
    "#select subset og 8,9,13,14\n",
    "arr_2d[1:-2,-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 20, 21, 22, 23, 24])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_2d[arr_2d>18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### modifying array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,\n",
       "        90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,\n",
       "        90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,\n",
       "        90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  90,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_arr[:50] = 90\n",
    "my_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9,  10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,\n",
       "        22,  23,  24,  25,  26,  27,  28,  29,  30,  31,  32,  33,  34,\n",
       "        35,  36,  37,  38,  39, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "       100, 100])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr4 = np.arange(9,50)\n",
    "new_arr4 = arr4[-10:] #this is a pointer of the original array\n",
    "arr4, new_arr4\n",
    "new_arr4[:] = 100\n",
    "arr4 # the original array being modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy array to avoid original overriding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr4 = np.arange(0,26)\n",
    "new_arr4 = arr4.copy() # explicitly create a copy\n",
    "new_arr4[-10:] = 100 # with this being broadcasted, it wont affect the original array\n",
    "new_arr4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARRAY OPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-134-919cdd352ce4>:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arr5/arr5 #python will still run and replace the invalid div with nan value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr5 = np.arange(0,25).reshape(5,5)\n",
    "arr5 +2\n",
    "arr5 * arr5\n",
    "#here comes the special\n",
    "arr5/arr5 #python will still run and replace the invalid div with nan value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(arr5)\n",
    "np.sin(arr5)\n",
    "np.log(arr5) #this is logaritmic\n",
    "np.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10,  35,  60,  85, 110])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr5.sum(axis=0) #across the row\n",
    "arr5.sum(axis=1) #across the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create column in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'] = np.vectorize(quality)(df['total_bill'],df['tip'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "note that quality must be defined first and it designed to take 2 arg.\n",
    "np vectorize generally will run faster than normal apply function or lambda operation.\n",
    "\n",
    "np vectorize is to transform a function that non numpy aware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistic With Numpy and Scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISTRIBUTION TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform Distribution"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is Uniform Distribution?\n",
    "In statistics, a type of probability distribution in which all outcomes are equally likely. A deck of cards has within it uniform distributions because the likelihood of drawing a heart, a club, a diamond or a spade is equally likely. A coin also has a uniform distribution because the probability of getting either heads or tails in a coin toss is the same.\n",
    "\n",
    "The uniform distribution can be visualized as a straight horizontal line, so for a coin flip returning a head or tail, both have a probability p = 0.50 and would be depicted by a line from the y-axis at 0.50."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "KEY TAKEAWAYS\n",
    "Uniform distributions are probability distributions with equally likely outcomes.\n",
    "There are two types of uniform distributions: discrete and continuous. In the former type of distribution, each outcome is discrete. In a continuous distribution, outcomes are continuous and infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Distribution"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is Normal Distribution?\n",
    "Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "KEY TAKEAWAYS\n",
    "A normal distribution is the proper term for a probability bell curve.\n",
    "In a normal distribution the mean is zero and the standard deviation is 1. It has zero skew and a kurtosis of 3.\n",
    "Normal distributions are symmetrical, but not all symmetrical distributions are normal.\n",
    "In reality, most pricing distributions are not perfectly normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.8\n",
      "85.8\n",
      "8.16\n"
     ]
    }
   ],
   "source": [
    "grades = [88, 82, 85, 84, 90]\n",
    "mean = np.mean(grades)\n",
    "\n",
    "difference_one = (88 - mean) **2\n",
    "difference_two = (82 - mean) **2\n",
    "difference_three = (85 - mean) **2\n",
    "difference_four = (84 - mean) **2\n",
    "difference_five = (90 - mean) **2\n",
    "\n",
    "#Part 1: Sum the differences\n",
    "difference_sum = float(difference_one + difference_two + difference_three+ difference_four + difference_five)\n",
    "\n",
    "#Part 2: Average the differences\n",
    "average_difference = difference_sum / 5\n",
    "print(difference_sum)\n",
    "print(mean)\n",
    "print(average_difference)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Variance In NumPy\n",
    "Well done! You’ve calculated the variance of a data set. The full equation for the variance is as follows:\n",
    "\n",
    "Let’s dissect this equation a bit.\n",
    "\n",
    "Variance is usually represented by the symbol sigma squared.\n",
    "We start by taking every point in the dataset — from point number 1 to point number N — and finding the difference between that point and the mean.\n",
    "Next, we square each difference to make all differences positive.\n",
    "Finally, we average those squared differences by adding them together and dividing by N, the total number of points in the dataset.\n",
    "All of this work can be done quickly using Python’s NumPy library. The var() function takes a list of numbers as a parameter and returns the variance of that dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By finding the number of standard deviations a data point is away from the mean, we can begin to investigate how unusual that datapoint truly is. In fact, you can usually expect around 68% of your data to fall within one standard deviation of the mean, 95% of your data to fall within two standard deviations of the mean, and 99.7% of your data to fall within three standard deviations of the mean."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "quartile, in general a data divide by four section or with three quartile in order to interpret it easily\n",
    "there are q1,q2,q3.\n",
    "median of a data = Q2\n",
    "\n",
    "two methods to define q1 and q3\n",
    "method 1 = exclude q2 to find q1 and q3\n",
    "method 2 = include q2 in both section to find q1 and q3\n",
    "\n",
    "meanwhile q2 is a median of a dataset. numpy has a powerful function to determine q1 and q3\n",
    "with np.quantile(dataset, 0-1) we can determine q1 with 0.25 and q3 with 0.75"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "suppose we want to divide data into 10 slice equally, or as 10 quantile\n",
    "formula :\n",
    "np.quantile(dataset, [ i/10.0 for i in range(1,10)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Nice work! Here are some of the major takeaways about quantiles:\n",
    "\n",
    "Quantiles are values that split a dataset into groups of equal size.\n",
    "If you have n quantiles, the dataset will be split into n+1 groups of equal size.\n",
    "The median is a quantile. It is the only 2-quantile. Half the data falls below the median and half falls above the median.\n",
    "Quartiles and percentiles are other common quantiles. Quartiles split the data into 4 groups while percentiles split the data into 100 groups."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sample Mean and Population Mean\n",
    "Suppose you want to know the average height of an oak tree in your local park. On Monday, you measure 10 trees and get an average height of 32 ft. On Tuesday, you measure 12 different trees and reach an average height of 35 ft. On Wednesday, you measure the remaining 11 trees in the park, whose average height is 31 ft. Overall, the average height for all trees in your local park is 32.8 ft.\n",
    "\n",
    "The individual measurements on Monday, Tuesday, and Wednesday are called samples. A sample is a subset of the entire population. The mean of each sample is the sample mean and it is an estimate of the population mean.\n",
    "\n",
    "Note that the sample means (32 ft., 35 ft., and 31 ft.) were all close to the population mean (32.8 ft.), but were all slightly different from the population mean and from each other.\n",
    "\n",
    "For a population, the mean is a constant value no matter how many times it’s recalculated. But with a set of samples, the mean will depend on exactly what samples we happened to choose. From a sample mean, we can then extrapolate the mean of the population as a whole. There are many reasons we might use sampling, such as:\n",
    "\n",
    "We don’t have data for the whole population.\n",
    "We have the whole population data, but it is so large that it is infeasible to analyze.\n",
    "We can provide meaningful answers to questions faster with sampling.\n",
    "When we have a numerical dataset and want to know the average value, we calculate the mean. For a population, the mean is a constant value no matter how many times it’s recalculated. But with a set of samples, the mean will depend on exactly what samples we happened to choose. From a sample mean, we can then extrapolate the mean of the population as a whole."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keynotes : \n",
    "MORE LARGER THE SAMPLE, MORE ACCURACY IT GETS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hypothesis is premise or claim that we want to test\n",
    "H0, null hipothesis : default or something that already . currently accepted value for a parameter\n",
    "    usually something that stable. such as mean, previous study.\n",
    "H1, alternative hypothesi : a new guy that want to challenge null hypothesis, or new statement that will\n",
    "    replace null hypothesis and involve claims to be tested.\n",
    "\n",
    "general rules of H0 and H1.\n",
    "- they are mathematicaly opposite\n",
    "- H0 is thought to be true until the evidence is enough to reject it. \n",
    "\n",
    "possible outcome of this test/insvestigation:\n",
    "1. we can reject H0, H1 will be the winner.\n",
    "2. fail to reject H0, H0 will remain used.\n",
    "3. type 1 error : rejecting null hipotesis but the reality is True.\n",
    "4. type 2 error : retaining null hipotesis but the reality is False.\n",
    "\n",
    "how do we do the test? we will use test statistic.\n",
    "test statistic is :\n",
    " - calculate from sample data used to decide\n",
    "\n",
    "statisticly significant.\n",
    "    is where we draw the line to help make decission.\n",
    "\n",
    "simple example :\n",
    "suppose a candy machine produce 5 gram of candy in a ten years, and after some workers doing maintenance\n",
    "the candy machine will no longer produce 5 gram anymore.\n",
    "H0 : avg = 5 gram\n",
    "H1 : avg != 5 gram\n",
    "\n",
    "test statistic :\n",
    "- collect 50 sample of candy per day.\n",
    "- get average value\n",
    "- calculate test statistic to help you determine is the data that you have statisticly significant enough\n",
    "    to reject H0 or not.\n",
    "\n",
    "-----------------\n",
    "on monday : avg value is 5.12 grams\n",
    "on wednesday : avg value is 5.72 grams < no longer close to 5 and close to 6 at this point \n",
    "    we start to doubt the H0 is true\n",
    "on friday : avg value is 7.27 grams < far above 5 and personaly i will reject H0 is true\n",
    "\n",
    "with this sampling example, there will be abigous tought and have no standard and based on feeling or perspective to reject the H0 or not.but in statistic have a concrete way to decide when we reject H0 and keep the H0.\n",
    "\n",
    "level of confidence = C : 95% or 99%\n",
    "    how confident we are in our decission. no one gonna believe you if your confidence is only 50 %?\n",
    "    so keep the level of C in 95 % or more.\n",
    "\n",
    "level of significance = alpha : 1- Level of Confidence (95%)\n",
    "                        alpha : 1 - C (0.95)\n",
    "                        alpha : 0.05\n",
    "\n",
    "P-value = probability of obtaining a sample more extreme than the ones observed in your data assuming H0 is true.\n",
    "conclussion of P test.\n",
    "1. if P value <= alpha : reject H0\n",
    "2. if p value > alpha : fail to reject H0\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Assumptions of T-Tests and ANOVA\n",
    "Before we use one or two sample t-tests or ANOVA, we need to be sure that the following things are true:\n",
    "\n",
    "1. The sample(s) should be normally distributed…ish\n",
    "Data analysts in the real world often still perform t-tests or ANOVAs on data that are not normally distributed. This is usually not a problem if sample size is large, but it depends on how non-normal the data is. In general, the bigger the sample size, the safer you are!\n",
    "\n",
    "2. The standard deviations of the samples should be equal\n",
    "For ANOVA and 2-Sample T-Tests, using datasets with standard deviations that are significantly different from each other will often obscure the differences in group means. That said, there is also a way to run a 2-Sample T-Test without assuming equal standard deviations (for example, by setting the equal_var parameter in the scipy.stats.ttest_ind() function equal to False). Running the test in this way has some disadvantages (it essentially makes it harder to reject the null hypothesis even when there is a true difference between groups), so it’s important to check for equal standard deviations before running a test.\n",
    "\n",
    "To check this assumption, it is normally sufficient to divide the two standard deviations and see if the ratio is “close enough” to 1. “Close enough” may differ in different contexts but generally staying within 10% should suffice. This equates to a ratio between 0.9 and 1.1.\n",
    "\n",
    "3. The samples must be independent\n",
    "When comparing two or more datasets, the values in one distribution should not affect the values in another distribution. In other words, knowing more about one distribution should not give you any information about any other distribution.\n",
    "\n",
    "Here are some examples where it would seem the samples are not independent:\n",
    "\n",
    "the number of goals scored per soccer player before, during, and after undergoing a rigorous training regimen\n",
    "a group of patients’ blood pressure levels before, during, and after the administration of a drug\n",
    "It is important to understand your datasets before you begin conducting hypothesis tests on them so that you know you are choosing the right test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STATISTIC FORMULA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "std_formula = variance ** 0.5\n",
    "nba_difference = datapoint - mean\n",
    "num_nba_deviations_distance = nba_difference / nba_standard_deviation\n",
    "    > this is the formula to find the std distance of a piece of data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STATISTIC FUNCTION "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy.stat import ttest_1samp\n",
    "    > for 1 sample H testing\n",
    "syntax = tstat,pval = ttest_1samp(dataset, expected mean(H1))\n",
    "syntax2 = tsat,pval = ttest_ind(data_sample1, data_sample2)\n",
    "    > for testing 2 data sample\n",
    "    > danger or multiple independent 2 sample test is :\n",
    "        error probability, (1 - (0.95 ** number of test being done))\n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "    > ANOVA test\n",
    "syntax = tstat, pval = f_oneway(data_sample1,data_sample2,data_sample3)\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "TUKEY TEST SYNTAX\n",
    "v = np.concatenate([a, b, c])\n",
    "labels = ['a'] * len(a) + ['b'] * len(b) + ['c'] * len(c)\n",
    "\n",
    "tukey_results = pairwise_tukeyhsd(v,labels,0.05)\n",
    "\n",
    "from scipy.stat import binom_test\n",
    "    > binom test\n",
    "syntax : binom_test(x, n, p)\n",
    "x is the number of “successes” (0.059 * 10000 in this case)\n",
    "n is the number of samples (10000 in this case)\n",
    "p is the expected percentage of successes (0.06 in this case)\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "    > chi square test\n",
    "syntax : chi2, pval, dof, expected = chi2_contingency(X)\n",
    "\n",
    "The input to chi2_contingency is a contingency table where:\n",
    "\n",
    "The columns are each a different condition, such as Interface A vs. Interface B\n",
    "The rows represent different outcomes, like “Clicked a Link” vs. “Didn’t Click”\n",
    "This table can have as many rows and columns as you need.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BINOMIAL TEST CASE EXAMPLE\n",
    "\n",
    "Suppose the goal of VeryAnts’s marketing team this quarter was to have 6% of customers click a link that was emailed to them. They sent out a link to 10,000 customers and 510 clicked the link, which comes out to 5.1% instead of 6%. Did they do significantly worse than the target? Let’s use a binomial test to answer this question.\n",
    "\n",
    "Use SciPy’s binom_test function to calculate the p-value the experiment returns for this distribution, where we wanted the mean to be 6% of emails opened, or p=0.06, but only saw 5.1% of emails opened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Test Methods"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                                Numerical\t|   Categorical\n",
    "Sample vs. Known Quantity\t1 Sample T-Test\t|   Binomial Test\n",
    "2 Samples\t                2 Sample T-Test |    Chi Square\n",
    "More Than 2 Samples\t          ANOVA\n",
    "                              and/or\n",
    "                              Tukey\t             Chi Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLICATION OF TEST METHODS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Let’s say that last month 7% of free users of a site converted to paid users, but this month only 5% of free users converted. What kind of test should we use to see if this difference is significant?\n",
    "- ANSWER =  chi square because the argument takes an array. column 1 is month, column 2 is mean of users\n",
    "\n",
    "2.You regularly order delivery from two different Pho restaurants, “What the Pho” and “Pho Tonic”. You want to know if there’s a significant difference between these two restaurants’ average time to deliver to your house. What test could you use to determine this?\n",
    "- ANSWER = 2 sample ttest. because we have a sample of 2 different group and measure its mean difference significant or not\n",
    "\n",
    "3.You just bought a new tea kettle that is supposed to heat water to boiling in 2 minutes. What kind of test can you run to determine if the time-to-boil is averaging significantly more than 2 minutes?\n",
    "- ANSWER = 1 sample ttest because with one sample of one single group \n",
    "\n",
    "You’ve surveyed 10 people who work in finance, 10 people who work in education, and 10 people who work in the service industry on how many cups of coffee they drink per day. What test can you use to determine if there is a significant difference between the average coffee consumption of these three groups?\n",
    "-ANSWER : ANOVA, this test only showing there is a significant difference or not. instead of which pair of sample can rejecet H0\n",
    "\n",
    "5. You own a juice bar and you theorize that 75% of your customers live in the surrounding 5 blocks. You survey a random sample of 12 customers and find that 7 of them live within those 5 blocks. What test do you run to determine if your results significantly differ from your expectation?\n",
    "-ANSWER : BINOMIAL TEST\n",
    "\n",
    "6.You regularly order delivery from two different Pho restaurants, “What the Pho” and “Pho Tonic”. You want to know if there’s a significant difference between these two restaurants’ average time to deliver to your house. What test could you use to determine this?\n",
    "-ANSWER : 2 SAMPLE TTEST\n",
    "\n",
    "7.What kind of test would you use to see if men and women identify differently as “Republican”, “Democrat”, or “Independent”?\n",
    "-ANSWER : CHI SQUARE\n",
    "\n",
    "8.You’ve collected data on 1000 different sites that end with .com, .edu, and .org and have recorded the number of each that have Times New Roman, Helvetica, or another font as their main font. What test can you use to determine if there’s a relationship between top-level domain and font type?\n",
    "- ANSWER : CHI SQUARE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Determination Calculator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "link survey calculator: https://content.codecademy.com/courses/learn-hypothesis-testing/margin_of_error/index.html\n",
    "\n",
    "link AB test sample calculator: https://content.codecademy.com/courses/learn-hypothesis-testing/a_b_sample_size/index4.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2 common sample determination calculator:\n",
    "    a. AB testing sample calculator\n",
    "    b. survey sample calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AB testing sample calculator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keynotes on doing AB testing\n",
    "- before conducting test, set the success indicator from the test. for the best result set only one target. althoough target can be set more than one.\n",
    "    example: - increasing viewer\n",
    "    - increasing engagement\n",
    "    - increasing revenue\n",
    "    - gaining more member to signup\n",
    "    - finding best product version based on above target\n",
    "\n",
    "- set data structure or designing data.\n",
    "    > in SQL might be creating what column to be filled while mining the data\n",
    "    > planning data type to be tested on hypo testing\n",
    "\n",
    "- calculating sample size\n",
    "    > sample size determination can be calculated with this 3 things\n",
    "    1. baseline conversion rate\n",
    "    2. minimum desired lift\n",
    "    3. significant treshold\n",
    "\n",
    "- DANGER OF PREDETERMINED SAMPLE\n",
    "Brian the Product Manager has been running an A/B Test for a redesign of Viral Villanelle’s landing page. Brian used the principles in the Sample Size Determination course on Codecademy to calculate a sample size. He needs 1,100 users to view each variant of the landing page in order to be able to detect his desired lift. When he reaches a total of 2,200 visits to both variants, he runs a Chi-Square test. The new website design performs slightly better, but the results are not statistically significant. Brian decides to run the test for another week to see if he can get to significance. He really wants to launch the redesigned website and he needs statistical validation to show to his boss.\n",
    "\n",
    "Brian has made a big mistake! By choosing to extend the A/B test past the sample size he needs, he introduces personal bias to the results of the test.\n",
    "\n",
    "If the results had already been significant, he wouldn’t have run the test any longer. If he continues this pattern of preferentially extending the test when he wants a different answer, he will be more likely to get the results he wants, regardless if these desired results reflect reality.\n",
    "\n",
    "It’s sad, but Brian will need to accept that the redesigned website isn’t significantly better than the original website.\n",
    "\n",
    "Here are two important rules for making sure that A/B tests remain unbiased:\n",
    "\n",
    "Don’t continue to run the test after the predetermined sample size, until “significant” results are found.\n",
    "Don’t stop a test before reaching the predetermined sample size, just because your results reach significance early (unless there are ethical reasons that require you to stop, like a prescription drug trial).\n",
    "Test data is sensitive to changes in sample size, which is why it is important to calculate beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline conversion rate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The baseline conversion rate is our estimate for the percent of people who will buy a shirt under the current website design. This number may be written as a proportion (eg., .5) or a percent (eg., 50%).\n",
    "\n",
    "We can generally calculate a baseline by looking at historical data for the option that we’re currently using.\n",
    "example: \n",
    "number_of_site_visitors = 2000.0\n",
    "number_of_converted_visitors = 1300.0\n",
    "\n",
    "# Calculate the conversion rate in terms of the above two variables here\n",
    "conversion_rate = number_of_converted_visitors/number_of_site_visitors * 100.0 \n",
    " > 65%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Desired lift"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Lift is generally expressed as a percent of the baseline conversion rate. Suppose that 6% of our customers currently buy socks on our website Sock Hops (that’s our baseline conversion rate). We think that a new website layout would increase this. Changing a website layout is hard, so we only think that it’s worth doing if at least 8% of our customers would buy socks on Sock Hops with the new layout. That means that we want to increase our conversions by 2%. To calculate lift:\n",
    "\n",
    "100 * (new - old) / old\n",
    "100 * (8 - 6) / 6\n",
    "33%\n",
    "Sock Hops’ desired lift is 33%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting the sample"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "suppose we have x1 as current headline for our . per day we have 350 users in our site\n",
    "we want to release new headline called x2 and want to see if it is better than the original or the current headline?\n",
    "\n",
    "suppose sample needed calculator shows we need 910 for each version. for somereason we split the traffic for the test to 80 for x1/ 20 for x2\n",
    "\n",
    "each day 350 users, then 80% of it is 210 and 20% of it is 70.\n",
    "to reach 910 users each version. x1 will take 4-5 days, while x2 will take 13 days\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### Sample Size calculator for survey"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3 numbers required for the calculator:\n",
    "a. margin of error\n",
    "b. population size\n",
    "c. confidence level\n",
    "d. likely "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Margin of Error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "margin of errors is a value to represent uncertainty of true result. the smaller margin is the more confident we are and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Population Size"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if population size is not has certain number or cant be predicted or keep increasing.\n",
    "SET THE POPULATION SIZE PARAMS TO 100.000 WILL SUFFICE AND ANY CHANGE ABOVE THAT NUMBER WILL NEGLIGIBLE\n",
    "\n",
    "if population size is certain or well knowed or steady enough\n",
    "SET THE POPULATION SIZE PARAM TO THAT NUMBER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Likely sample proportion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Often, before we conduct a survey, we have a guess of what we expect the results to be. This guess could be based upon the results from a previous survey, or perhaps the results of a small pilot study before the real study.\n",
    "\n",
    "As the expected proportion of people with the desired trait decreases, we can survey fewer people. For example, if we are projecting election results and Candidate C has 1% of the voter base, taking a small sample of only 5 people might be fine, because it is very likely that no one we have chosen is voting for Candidate C. This is close enough to the true proportion.\n",
    "\n",
    "As the expected proportion increases, it is rarer that we hit that proportion accurately with the random sample we choose.\n",
    "\n",
    "If we do not have historical data, we normally use 50%, which gives the most conservative (i.e., largest required) sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using survey calculator to perform AB test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "we want to make a comparison: Are girls more likely to want to learn Python than boys are?\n",
    "\n",
    "This survey is more similar to an A/B Test. Our baseline is the approximate percent of boys who want to learn Python, and our desired lift is the minimum difference between boys and girls that we want to be able to detect.\n",
    "\n",
    "Whenever we want to make comparisons between subpopulations in our survey, we can use the A/B Test Calculator in order to get our desired survey size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, linear regression is a linear approach to modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scalar response is something that we are trying to predict.\n",
    "explanatory variable is feature that might affect prediction of scalar response"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in general the mathematical equation of simple linear regression :\n",
    "y = mx + b\n",
    "\n",
    "y = dependent variable we are trying to predict\n",
    "x = feature or independent variable that might affecting prediction\n",
    "m = slope\n",
    "b = y intercept\n",
    "\n",
    "in generalize form, this is the same with:\n",
    "y = B0 + B1X"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Linear regression has many practical uses. Most applications fall into one of the following two broad categories:\n",
    "\n",
    "If the goal is prediction, forecasting, or error reduction,linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.\n",
    "\n",
    "If the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "source : wikipedia/linear regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Simple linear regression lives up to its name: it is a very straightforward\n",
    "simple linear regression approach for predicting a quantitative response Y on the basis of a single predictor variable X. It assumes that there is approximately a linear relationship between X and Y . Mathematically, we can write this linear relationship as\n",
    "\n",
    "Y ≈ β 0 + β 1 X. (3.1)\n",
    "\n",
    "You might read “≈” as “is approximately modeled as”. We will sometimes describe (3.1) by saying that we are regressing Y on X (or Y onto X). For example, X may represent TV advertising and Y may represent sales .Then we can regress sales onto TV by fitting the model\n",
    "\n",
    "sales ≈ β 0 + β 1 × TV.\n",
    "\n",
    "In Equation 3.1, β 0 and β 1 are two unknown constants that represent the intercept and slope terms in the linear model. Together, β 0 and β 1 are intercept and slope.\n",
    "known as the model coefficients or parameters. Once we have used our coefficient parameter training data to produce estimates\n",
    "\n",
    "β 0 and β 1 for the model coefficients, we can predict future sales on the basis of a particular value of TV advertising\n",
    "by computing\n",
    "\n",
    "y(hat) = β 0 + β 1 x, (3.2)\n",
    "where ˆ y indicates a prediction of Y on the basis of X = x. Here we use ahat symbol, ˆ \n",
    "to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "source : ISLR 61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary :  \n",
    "y = b0 + b1x + e  \n",
    "y_hat = b0 +b1x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why is that? how that can be different ?  \n",
    "e = error term  \n",
    "the true y value is resulted by adding error/residual value from prediction. on the other hand, when we predicting y value in y_hat we dont know the error value until we compare it to the y test.\n",
    "and usually, e is assumed to be zero."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ORDINARY LEAST SQUARE (OLS)\n",
    "Pengertian OLS (Ordinary Least Square) adalah suatu metode ekonometrik dimana terdapat variable independen yang merupakan variable penjelas dan variable dependen yaitu variable yang dijelaskan dalam suatu persamaan linier. ... OLS merupakan metode regresi yang meminimalkan jumlah kesalahan (error) kuadrat.\n",
    "Dalam model regresi linear sederhana untuk mengetahui hubungan antara dua variabel yang salah satu variabel menjadi variabel dependent (tak bebas) dan variabel lainnya independent (variabel bebas). Dalam analisis regresi linear, hasil akhir yang diperoleh adalah fungsi regresi populasi yang didapat dari fungsi regresi sampel yang nantinya dapat digunakan untuk estimasi.\n",
    "\n",
    "Ada beberapa metode yang dapat digunakan untuk mengestimasi fungsi regresi, salah satunya adalah OLS (Ordinary Linear Square). OLS merupakan metode estimasi fungsi regresi yang paling sering digunakan. Kriteria OLS adalah \"Line of Best Fit\" atau dengan kata lain jumlah kuadrat dari deviasi antara titik-titik observasi dengan garis regresi adalah minimum. \n",
    "Dalam model regresi linear memiliki beberapa asumsi dasar yang harus dipenuhi untuk menghasilkan estimasi yang BLUE, yaitu : Homoscedastic, no-multicollinearity dan no-autocorrelation.\n",
    "\n",
    "Adapun estimator yang BLUE, adalah\n",
    "1.\tBest, hasil model regresi adalah terbaik dan menghasilkan error yang kecil.\n",
    "2.\tLinear, model yang digunakan dalam regresi sesuai kaidah model OLS yaitu linear dan pangkat variabel-variabelnya paling tinggi adalah satu\n",
    "3.\tUnbiased, nilai yang diharapkan (hasil estimasi menggunakan model regresi) sama dengan nilai yang benar\n",
    "4.\tEstimator, model regresi yang terbentuk memiliki varians yang minimal dari estimator lainnya.\n",
    "Asumsi-asumsi yang BLUE :\n",
    "1.\tModel regresi adalah linear dalam parameter\n",
    "2.\tError term berdistribusi normal, implikasinya Y dan distribusi sampling koefisien regresi memiliki distribusi normal. Sehingga nilai harapan dan rata-rata kesalahan (error) adalah nol.\n",
    "3.\tVarians tetap (homoscedastic)\n",
    "4.\tTidak ada hubungan variabel bebas dengan error term\n",
    "5.\tTidak ada autocorrelation antara error term\n",
    "6.\tPada regresi linear berganda hubungan antarvariabel bebas (multicolinearity) tidak terjadi.\n",
    "Hasil estimasi yang bersifat BLUE, sebagai berikut :\n",
    "1.\tEfisien, hasil nilai estimasi memiliki varians yang minimum dan tidak bias\n",
    "2.\tTidak bias (unbiased), hasil estimasi sesuai dengan parameter\n",
    "3.\tKonsisten, jika ukuran sampel ditambah tanpa batas, maka hasil nilai estimasi akan mendekati parameter populasi sebenarnya. (jika memenuhi asumsi normal, dimana error term berdistribusi normal standar dengan mena nol dan standar deviasi satu)\n",
    "4.\tIntercept nilai dependent saat nilai independent nol memiliki distribusi normal\n",
    "5.\tKoefisien regresi akan memiliki distribusi normal\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "source : DR Harjanto Sutejo S.SI., MMSI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is an extension of simple linear regression. have more than one independent variable or predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLR make things little bit complicated. infact there will be/create relationship between variables that might affecting prediction.  \n",
    "MLR may address some issue:\n",
    "1. overfitting\n",
    "2. multirelationship among variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "general formula :  \n",
    "y = b0 + b1x1 + b2x2 +.... +e  \n",
    "y_hat = y = b0 + b1x1 + b2x2 +.... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiple regression model is based on the following assumptions:\n",
    "\n",
    "- There is a linear relationship between the dependent variables and the independent variables.  \n",
    "- The independent variables are not too highly correlated with each other.  \n",
    "- yi observations are selected independently and randomly from the population.  \n",
    "- Residuals should be normally distributed with a mean of 0 and variance σ.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most common evaluation metrics for regression:\n",
    "1. Mean Absolute error\n",
    "2. Mean Square Error\n",
    "3. Root Mean Square Erro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mean Absolute Error\n",
    "- easy to understand\n",
    "- average of all predicted rows\n",
    "- wont punish large error, see anscombe's quartet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Mean Square Error\n",
    "- punish more large error \n",
    "- avoid anscombe quartet\n",
    "- hard to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Root Mean Square Error\n",
    "- fix MSE y value\n",
    "- fix punished large value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context of Performance Error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "performace metrics return a value that measure your error.\n",
    "10$ is a very small error against million dollar data. but it is a huge error against candy bar price\n",
    "\n",
    "compare error metric with average label value to get intuition of its overall performance.\n",
    "domain expertise is huge factor too\n",
    "\n",
    "comparing previous model work to measure improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "often it is a good idea to separate residual into new column or separated data. recall that residual is (y - y_hat) true y test value substracted by y prediction. this is why\n",
    "1. determine linear regression is appropriate or not in our dataset\n",
    "> famous anscombe quartet is not valid for linear regression, this is why residual plot comes in handy to judge that  \n",
    "\n",
    "2. not matter how many feature do you have, plotting residual will give insight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot residual error to:\n",
    "1. distribution plot, on its residual column\n",
    "2. scatter plot, on true y test value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "residual error should be random and close to normal distribution with mean close to zero.  \n",
    "because we want them to be close to zero which mean our prediction is more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when plotting residual error against y true value, we set red line on 0 y axis.\n",
    "the perfect fit condition is when your residual datapoint is on red line or nearby it. also, it is not creating a pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Beta Coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Coefisien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the coefficients:\n",
    "\n",
    "---\n",
    "* Holding all other features fixed, a 1 unit (A thousand dollars) increase in TV Spend is associated with an increase in sales of  0.045 \"sales units\", in this case 1000s of units . \n",
    "* This basically means that for every $1000 dollars spend on TV Ads, we could expect 45 more units sold.\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "* Holding all other features fixed, a 1 unit (A thousand dollars) increase in Radio Spend is associated with an increase in sales of  0.188 \"sales units\", in this case 1000s of units . \n",
    "* This basically means that for every $1000 dollars spend on Radio Ads, we could expect 188 more units sold.\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Holding all other features fixed, a 1 unit (A thousand dollars) increase in Newspaper Spend is associated with a **decrease** in sales of  0.001 \"sales units\", in this case 1000s of units . \n",
    "* This basically means that for every $1000 dollars spend on Newspaper Ads, we could actually expect to sell 1 less unit. Being so close to 0, this heavily implies that newspaper spend has no real effect on sales.\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note! In this case all our units were the same for each feature (1 unit = $1000 of ad spend). But in other datasets, units may not be the same, such as a housing dataset could try to predict a sale price with both a feature for number of bedrooms and a feature of total area like square footage. In this case it would make more sense to *normalize* the data, in order to clearly compare features and results. We will cover normalization later on.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "polynomial is an enhancement of linear regression to maximize its performance.  \n",
    "the main idea why using polynomial regression is trying to reveal potential relationship between predictor variables.  \n",
    "this called interaction terms  \n",
    "or sometimes our model is not perform best fit to data which resulting less precision prediction (high bias).  \n",
    "when data behave like logistic equation (not actually logistic, but behave like that) we need more powerful model to fit the data at its best.  \n",
    "according to explanation above, this is why polynomial regression comes in handy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remember, not all dataset can be applied with polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "polynomial equation :  \n",
    "$$\\hat{y} = \\beta_0 + \\beta_1x_1 + \\epsilon $$\n",
    "\n",
    "and create more features from the original x feature for some *d* degree of polynomial.\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_1x^2_1 +  ... + \\beta_dx^d_1 + \\epsilon$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this might seems identical to multiple regression formula, but this is not the same.  \n",
    "one of the main actor here is polynomial degree (d).\n",
    "default degree in sklearn is 2, which means in example of 2 variables (a and b) resulting feateures:  \n",
    "$$[1, a, b, a^2, ab, b^2] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since only x is being squared, polynomial still considered as linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue of Polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this might address to us:\n",
    "1. overfitting\n",
    "2. not creating useful prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Polynommial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science path lesson.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. the model fit too much noise in the data\n",
    "2. this often results in low error on training set but high error on test/ validation sets\n",
    "3. this might perform well in the training data, but it is a huge problem in test/ unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. model doesnt fit the data well enough\n",
    "2. low variance but high bias\n",
    "3. simple model often resulting an underfit\n",
    "4. generalize too much\n",
    "5. poor performance in training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing best degree of Polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in theory, more degree means more fit and capture data noise in detailed manner.  \n",
    "but this could be dangerous and very poor performance with unseen data.\n",
    "\n",
    "choosing perfect degree in polynomial is necessarry to get best fit, low bias, and can predict well for upcoming unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Choosing a Model\n",
    "\n",
    "### Adjusting Parameters\n",
    "\n",
    "Are we satisfied with this performance? Perhaps a higher order would improve performance even more! But how high is too high? It is now up to us to possibly go back and adjust our model and parameters, let's explore higher order Polynomials in a loop and plot out their error. This will nicely lead us into a discussion on Overfitting.\n",
    "\n",
    "Let's use a for loop to do the following:\n",
    "\n",
    "1. Create different order polynomial X data\n",
    "2. Split that polynomial data for train/test\n",
    "3. Fit on the training data\n",
    "4. Report back the metrics on *both* the train and test results\n",
    "5. Plot these results and explore overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularization seek to solve few common model issue by:\n",
    "1. minimizing model complexity\n",
    "2. penalzing the loss function\n",
    "3. reduc model overfitting (add more bias to reduce model variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization purpose is reduce model overfitting by requiring :\n",
    "1. more bias\n",
    "2. more search for hyperparams penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 main types of regularization: \n",
    "1. L1\n",
    "> lasso regression\n",
    "2. L2\n",
    "> ridge regression\n",
    "3. L1 and L2 combined\n",
    "> elastic net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 Regularization or Lasso regression.\n",
    "- limit the size of the coefficient\n",
    "- can yield sparse model where some coef can be zero\n",
    "\n",
    "with some features can become zero, you can decide which features is to be considered in regards to your model output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "formula :\n",
    "RSS + lambda(sum of beta coef)  \n",
    "RSS = Residual sum of square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common terms in ML\n",
    "\n",
    "MSE = Mean square error  \n",
    "RSS = residual sum of square\n",
    "\n",
    "error and residual are different.  \n",
    "error is the difference between y prediction against y true  \n",
    "residual is the difference of y against model line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 or Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add penalty equal to the square of the magnitude of coef.  \n",
    "- all coef are shrunk by the same factor\n",
    "- doesnt necessarily eliminate the coef\n",
    "\n",
    "formula :  \n",
    "RSS + lambda (sum of beta coef^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combined L1 and L2 with an addition of alpha that deciding portion of them.  \n",
    "alpha is between 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature scaling provides many benefits to our machine learning process.  \n",
    "some machine learning models that rely on distance metrics require scaling to perform well  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
